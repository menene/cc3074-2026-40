inspect(reglas[52])
plot(reglas)
plot(reglas, engine = 'plotly')
plot(reglas, engine = 'plotly')
plot(reglas, method = "graph",  engine = "htmlwidget")
inspect(reglas[13])
inspect(reglas[14])
inspect(reglas[52])
inspect(reglas[13])
inspect(reglas[14])
inspect(reglas[52])
inspect(reglas[189])
inspect(reglas[13])
inspect(reglas[14])
inspect(reglas[52])
inspect(reglas[189])
tinytex::tlmgr_install(c("babel-spanish", "enumitem", "hyperref", "geometry", "xcolor", "titlesec", "fancyhdr"))
lillie_results <- lapply(datos_cuant, lillie.test)
############################################################
# ANÁLISIS EXPLORATORIO - NORMALIDAD
# Dataset: Churn_Modelling.csv
############################################################
# 1. Cargar datos -------------------------------------------
#install.packages("nortest")
library(nortest)
churn <- read.csv("Churn_Modelling.csv")
setwd("~/Downloads")
library(nortest)
churn <- read.csv("Churn_Modelling.csv")
# Vista general
str(churn)
summary(churn)
nrow(churn)
clasificacion_variables <- data.frame(
Variable = c(
"RowNumber",
"CustomerId",
"Surname",
"CreditScore",
"Geography",
"Gender",
"Age",
"Tenure",
"Balance",
"NumOfProducts",
"HasCrCard",
"IsActiveMember",
"EstimatedSalary",
"Exited"
),
Tipo = c(
"Cualitativa categórica (ID)",
"Cualitativa categórica (ID)",
"Cualitativa categórica",
"Cuantitativa discreta",
"Cualitativa categórica",
"Cualitativa categórica",
"Cuantitativa continua",
"Cuantitativa discreta",
"Cuantitativa continua",
"Cuantitativa discreta",
"Cualitativa categórica (binaria)",
"Cualitativa categórica (binaria)",
"Cuantitativa continua",
"Cualitativa categórica (binaria)"
),
stringsAsFactors = FALSE
)
clasificacion_variables
clasificacion_variables
hist(churn$Age)
hist(churn$Balance)
hist(churn$EstimatedSalary)
hist(churn$CreditScore)
hist(churn$Tenure)
hist(churn$NumOfProducts)
View(churn)
View(churn)
hist(churn$Age)
churn <- read.csv("Churn_Modelling.csv")
# Vista general
str(churn)
summary(churn)
nrow(churn)
hist(churn$Age)
hist(churn$Balance)
hist(churn$EstimatedSalary)
hist(churn$CreditScore)
hist(churn$Tenure)
hist(churn$NumOfProducts)
boxplot(churn$Age)
boxplot(churn$Balance)
boxplot(churn$EstimatedSalary)
boxplot(churn$CreditScore)
boxplot(churn$Tenure)
# Vista general
str(churn)
############################################################
# ANÁLISIS EXPLORATORIO - NORMALIDAD
# Dataset: Churn_Modelling.csv
############################################################
# 1. Cargar datos -------------------------------------------
#install.packages("nortest")
library(nortest)
churn <- read.csv("Churn_Modelling.csv")
# Vista general
str(churn)
summary(churn)
nrow(churn)
# 2. Clasificación de variables -----------------------------
clasificacion_variables <- data.frame(
Variable = c(
"RowNumber",
"CustomerId",
"Surname",
"CreditScore",
"Geography",
"Gender",
"Age",
"Tenure",
"Balance",
"NumOfProducts",
"HasCrCard",
"IsActiveMember",
"EstimatedSalary",
"Exited"
),
Tipo = c(
"Cualitativa categórica (ID)",
"Cualitativa categórica (ID)",
"Cualitativa categórica",
"Cuantitativa discreta",
"Cualitativa categórica",
"Cualitativa categórica",
"Cuantitativa continua",
"Cuantitativa discreta",
"Cuantitativa continua",
"Cuantitativa discreta",
"Cualitativa categórica (binaria)",
"Cualitativa categórica (binaria)",
"Cuantitativa continua",
"Cualitativa categórica (binaria)"
),
stringsAsFactors = FALSE
)
clasificacion_variables
# 3. Histogramas -----------------------------
hist(churn$Age)
hist(churn$Balance)
hist(churn$EstimatedSalary)
hist(churn$CreditScore)
hist(churn$Tenure)
hist(churn$NumOfProducts)
# 4. Boxplots -----------------------------
boxplot(churn$Age)
boxplot(churn$Balance)
boxplot(churn$EstimatedSalary)
boxplot(churn$CreditScore)
boxplot(churn$Tenure)
boxplot(churn$NumOfProducts)
# 6. Separación de varibles numericas -----------------------
cuant_discretas <- c(
"CreditScore",
"Tenure",
"NumOfProducts"
)
cuant_continuas <- c(
"Age",
"Balance",
"EstimatedSalary"
)
cuantitativas <- c(cuant_discretas, cuant_continuas)
datos_cuant <- churn[, cuantitativas]
datos_cuant
# 7. Pruebas de normalidad: Shapiro-Wilk --------------------
# Solo funciona para muestras pequeñas
# seed para reproducibilidad
set.seed(123)
n_total <- nrow(churn)
n_train <- 5000
indices_train <- sample(1:n_total, size = n_train, replace = FALSE)
churn_sample <- churn[indices_train, ]
datos_cuant_sample <- churn_sample[, cuantitativas]
# Hipótesis:
# H0: La variable sigue una distribución normal
# H1: La variable NO sigue una distribución normal
shapiro_results <- lapply(datos_cuant_sample, shapiro.test)
# Mostrar resultados
shapiro_results
shapiro_results$Age
# p-value > 0.05 → No se rechaza H0 (normalidad)
# p-value ≤ 0.05 → Se rechaza H0 (normalidad)
# PRUEBA KS -----------------------------------------------
# Hipótesis:
# H0: La variable sigue una distribución normal
# H1: La variable NO sigue una distribución normal
ks_results <- lapply(datos_cuant, function(x) ks.test(x, "pnorm", mean(x), sd(x)))
# Mostrar resultados
ks_results
# Ejemplo
ks_results$Age
# p-value > 0.05 → No se rechaza H0
# p-value ≤ 0.05 → Se rechaza H0
# PRUEBA LILLIEFORS ---------------------------------------
# Hipótesis:
# H0: La variable sigue una distribución normal
# H1: La variable NO sigue una distribución normal
lillie_results <- lapply(datos_cuant, lillie.test)
# Mostrar resultados
lillie_results
# Ejemplo
lillie_results$Age
# p-value > 0.05 → No se rechaza H0
# p-value ≤ 0.05 → Se rechaza H0
# seed para reproducibilidad
set.seed(314159)
n_total <- nrow(churn)
n_train <- 5000
indices_train <- sample(1:n_total, size = n_train, replace = FALSE)
churn_sample <- churn[indices_train, ]
datos_cuant_sample <- churn_sample[, cuantitativas]
c
# Mostrar resultados
shapiro_results
shapiro_results$Age
ks_results <- lapply(datos_cuant, function(x) ks.test(x, "pnorm", mean(x), sd(x)))
# Mostrar resultados
ks_results
# Ejemplo
ks_results$Age
lillie_results <- lapply(datos_cuant, lillie.test)
# Mostrar resultados
lillie_results
# Ejemplo
lillie_results$Age
knitr::opts_chunk$set(echo = TRUE)
library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(fpc) #para hacer el plotcluster
library(NbClust) #Para determinar el número de clusters óptimo
library(factoextra) #Para hacer gráficos bonitos de clustering
library(hopkins) #Para revisar si vale la pena hacer agrupamiento
library(GGally) #Para hacer el conjunto de graficos
library(FeatureImpCluster) #Para revisar la importancia de las variables en los grupos.
library(pheatmap) #Para hacer mapa de calor
datos<-iris
set.seed(123)
datos<-datos[complete.cases(iris),]
summary(datos)
datos[,1:4]<-scale(datos[,1:4])
set.seed(123)
hopkins(datos[,1:4])
#Matriz de distancia
datos_dist<- dist(datos[,1:4])
fviz_dist(datos_dist, show_labels = F)
wss=0
for (i in 1:10)
wss[i] <- sum(kmeans(iris[,1:4], centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
fviz_nbclust(datos[,1:4], kmeans, method = "wss") +
labs(subtitle = "Elbow method")
fviz_nbclust(datos[,1:4], kmeans, method = "silhouette") +
labs(subtitle = "Silhouette method")
fviz_nbclust(datos[,1:4], kmeans,
nstart = 25, method = "gap_stat", nboot = 50, verbose = F)+
labs(subtitle = "Gap statistic method")
nb <- NbClust(datos[,1:4], distance = "euclidean", min.nc = 2,
max.nc = 10, method = "complete", index ="all")
km<-kmeans(datos[,1:4],3,iter.max =100)
datos$grupo<-km$cluster
km
plotcluster(datos[,1:4],km$cluster) #grafica la ubicaciÃ³n de los clusters
fviz_cluster(km, data = datos[,1:4],geom = "point", ellipse.type = "norm")
km$size
km$withinss
library(ggrepel)
m<-data.frame(withinss=km$withinss, size=km$size)
ggplot(m, aes(size,withinss))+
geom_point()+
geom_smooth(method="lm")+
labs(x="cardinalidad (size)",y="magnitud (whithinss)")+
geom_text_repel(label=rownames(m))
library(GGally)
datos$grupo<-as.factor(datos$grupo)
ggpairs(datos[,c(1:4,6)],
aes(col = factor(grupo)),
progress = FALSE)
library(flexclust)
set.seed(123)
res<-kcca(datos[,1:4],3)
importance <- FeatureImpCluster(res, as.data.table(datos[,1:4]))
plot(importance)
barplot(res, bycluster=T)
knitr::opts_chunk$set(echo = TRUE)
library(cluster) #Para calcular la silueta
library(e1071)#para cmeans
library(mclust) #mixtures of gaussians
library(fpc) #para hacer el plotcluster
library(NbClust) #Para determinar el número de clusters óptimo
library(factoextra) #Para hacer gráficos bonitos de clustering
library(hopkins) #Para revisar si vale la pena hacer agrupamiento
library(GGally) #Para hacer el conjunto de graficos
library(FeatureImpCluster) #Para revisar la importancia de las variables en los grupos.
library(pheatmap) #Para hacer mapa de calor
datos<-iris
set.seed(123)
datos<-datos[complete.cases(iris),]
summary(datos)
datos[,1:4]<-scale(datos[,1:4])
set.seed(123)
hopkins(datos[,1:4])
#Matriz de distancia
datos_dist<- dist(datos[,1:4])
fviz_dist(datos_dist, show_labels = F)
wss=0
for (i in 1:10)
wss[i] <- sum(kmeans(iris[,1:4], centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
fviz_nbclust(datos[,1:4], kmeans, method = "wss") +
labs(subtitle = "Elbow method")
fviz_nbclust(datos[,1:4], kmeans, method = "silhouette") +
labs(subtitle = "Silhouette method")
fviz_nbclust(datos[,1:4], kmeans,
nstart = 25, method = "gap_stat", nboot = 50, verbose = F)+
labs(subtitle = "Gap statistic method")
nb <- NbClust(datos[,1:4], distance = "euclidean", min.nc = 2,
max.nc = 10, method = "complete", index ="all")
km<-kmeans(datos[,1:4],3,iter.max =100)
datos$grupo<-km$cluster
km
plotcluster(datos[,1:4],km$cluster) #grafica la ubicaciÃ³n de los clusters
fviz_cluster(km, data = datos[,1:4],geom = "point", ellipse.type = "norm")
km$size
km$withinss
library(ggrepel)
m<-data.frame(withinss=km$withinss, size=km$size)
ggplot(m, aes(size,withinss))+
geom_point()+
geom_smooth(method="lm")+
labs(x="cardinalidad (size)",y="magnitud (whithinss)")+
geom_text_repel(label=rownames(m))
library(GGally)
datos$grupo<-as.factor(datos$grupo)
ggpairs(datos[,c(1:4,6)],
aes(col = factor(grupo)),
progress = FALSE)
library(flexclust)
set.seed(123)
res<-kcca(datos[,1:4],3)
importance <- FeatureImpCluster(res, as.data.table(datos[,1:4]))
plot(importance)
barplot(res, bycluster=T)
fviz_cluster(km,
datos[,1:4],
labelsize = 6,
choose.vars = c("Sepal.Width","Sepal.Length"),
main="k=3 grupos")
silkm<-silhouette(km$cluster,dist(datos[,1:4]))
mean(silkm[,3]) #0.45, no es la mejor particiÃ³n pero no estÃ¡ mal
plot(silkm, cex.names=.4, col=1:3)
matriz_dist<- dist(datos[,1:4])
hc<-hclust(datos_dist, method = "ward.D2") #Genera el clustering jerÃ¡rquico de los datos
plot(hc, cex=0.5, axes=FALSE) #Genera el dendograma
rect.hclust(hc,k=3)
fviz_dend(hc, k=3, rect = T, cex = .5)
groups<-cutree(hc,k=3) #corta el dendograma, determinando el grupo de cada fila
datos$gruposHC<-groups
table(datos$gruposHC)
by(datos[,1:4],datos[,"gruposHC"], colMeans)
fviz_dend(hc,k=3, cex = .2, horiz = T)
set.seed(123)
fviz_dend(hc,k=3, cex = .4, type = "circular", color_labels_by_k = T)
fviz_dend(hc, k=3, color_labels_by_k = T, cex = .7,type = "phylogenic", repel = T)
pheatmap(datos[,1:4],cluster_cols = F, scale = "none",cutree_rows = 3, fontsize = 6, clustering_distance_rows = "euclidean",clustering_method = "ward.D2")
silhc<-silhouette(groups,datos_dist)
mean(silhc[,3]) #0.45, no es la mejor particiÃ³n pero no estÃ¡ mal
plot(silhc, cex.names=.4, col=1:3)
mc<-Mclust(datos[,1:4],3)
summary(mc)
datos$mxGau<-mc$classification
silmg<-silhouette(mc$classification,datos_dist)
mean(silmg[,3])
plot(silmg, cex.names=.4, col=1:3)
table(datos$Species,datos$grupo)
table(datos$Species, datos$gruposHC)
table(datos$Species, datos$mxGau)
View(datos)
?iris
knitr::opts_chunk$set(echo = TRUE)
library(cluster)
library(e1071)
library(mclust)
library(fpc)
library(NbClust)
library(factoextra)
library(hopkins)
library(GGally)
library(FeatureImpCluster)
library(pheatmap)
datos <- iris
set.seed(123)
datos <- datos[complete.cases(iris),]
summary(datos)
datos <- iris
set.seed(314159)
datos <- datos[complete.cases(iris),]
summary(datos)
```{r escalado de los datos}
datos[,1:4] <- scale(datos[,1:4])
set.seed(314159)
hopkins(datos[,1:4])
datos_dist <- dist(datos[,1:4])
fviz_dist(datos_dist, show_labels = FALSE)
wss = 0
for (i in 1:10)
wss[i] <- sum(kmeans(iris[,1:4], centers=i)$withinss)
plot(1:10, wss, type="b",
xlab="Número de Clusters",
ylab="Suma de cuadrados dentro del grupo")
fviz_nbclust(datos[,1:4], kmeans, method = "wss")
fviz_nbclust(datos[,1:4], kmeans, method = "silhouette")
setwd("~/code/data/02-normalidad")
library(nortest)
churn <- read.csv("Churn_Modelling.csv")
# Vista general
str(churn)
nrow(churn)
clasificacion_variables <- data.frame(
Variable = c(
"RowNumber",
"CustomerId",
"Surname",
"CreditScore",
"Geography",
"Gender",
"Age",
"Tenure",
"Balance",
"NumOfProducts",
"HasCrCard",
"IsActiveMember",
"EstimatedSalary",
"Exited"
),
Tipo = c(
"Cualitativa categórica (ID)",
"Cualitativa categórica (ID)",
"Cualitativa categórica",
"Cuantitativa discreta",
"Cualitativa categórica",
"Cualitativa categórica",
"Cuantitativa continua",
"Cuantitativa discreta",
"Cuantitativa continua",
"Cuantitativa discreta",
"Cualitativa categórica (binaria)",
"Cualitativa categórica (binaria)",
"Cuantitativa continua",
"Cualitativa categórica (binaria)"
),
stringsAsFactors = FALSE
)
clasificacion_variables
hist(churn$Age)
boxplot(churn$Age)
cuant_discretas <- c(
"CreditScore",
"Tenure",
"NumOfProducts"
)
cuant_continuas <- c(
"Age",
"Balance",
"EstimatedSalary"
)
cuantitativas <- c(cuant_discretas, cuant_continuas)
datos_cuant <- churn[, cuantitativas]
datos_cuant
View(datos_cuant)
View(datos_cuant)
# seed para reproducibilidad
set.seed(314159)
n_total <- nrow(churn)
n_train <- 5000
indices_train <- sample(1:n_total, size = n_train, replace = FALSE)
churn_sample <- churn[indices_train, ]
datos_cuant_sample <- churn_sample[, cuantitativas]
View(datos_cuant)
View(datos_cuant_sample)
View(datos_cuant_sample)
shapiro_results <- lapply(datos_cuant_sample, shapiro.test)
# Mostrar resultados
shapiro_results
shapiro_results$Age
ks_results <- lapply(datos_cuant, function(x) ks.test(x, "pnorm", mean(x), sd(x)))
# Mostrar resultados
ks_results
# Ejemplo
ks_results$Age
lillie_results <- lapply(datos_cuant, lillie.test)
# Mostrar resultados
lillie_results
# Ejemplo
lillie_results$Age
knitr::opts_chunk$set(echo = TRUE)
library(nortest)
churn <- read.csv("Churn_Modelling.csv")
str(churn)
summary(churn)
nrow(churn)
?iris
